{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision \n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "import os\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = '/kaggle/input/hymenoptera-data/hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_N(model, criterion, optimizer, num_epochs=10):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:       \n",
    "            if phase == 'train':\n",
    "#                 scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':       #  不是训练的话就不用计算梯度\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:        # 保存最好的模型参数数据\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "#     model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练一： 冻结模型参数，运用L2正则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 0.5739 Acc: 0.7213\n",
      "val Loss: 0.2853 Acc: 0.9346\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 0.4401 Acc: 0.8033\n",
      "val Loss: 0.2888 Acc: 0.8758\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 0.4109 Acc: 0.8197\n",
      "val Loss: 0.1984 Acc: 0.9412\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.5035 Acc: 0.7459\n",
      "val Loss: 0.2038 Acc: 0.9346\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.4263 Acc: 0.7951\n",
      "val Loss: 0.1799 Acc: 0.9477\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.4295 Acc: 0.8197\n",
      "val Loss: 0.1803 Acc: 0.9346\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.3826 Acc: 0.8279\n",
      "val Loss: 0.2191 Acc: 0.9346\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.3991 Acc: 0.8033\n",
      "val Loss: 0.1618 Acc: 0.9477\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.5066 Acc: 0.7541\n",
      "val Loss: 0.1782 Acc: 0.9346\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.4311 Acc: 0.8279\n",
      "val Loss: 0.1601 Acc: 0.9477\n",
      "\n",
      "Training complete in 1m 5s\n",
      "Best val Acc: 0.947712\n"
     ]
    }
   ],
   "source": [
    "model_18 = models.resnet18(pretrained=True) \n",
    "\n",
    "for p,name in zip(model_18.parameters(),model_18.state_dict().keys()):\n",
    "    p.requires_grad=False\n",
    "    \n",
    "model_18.fc = nn.Linear(model_18.fc.in_features,2)\n",
    "\n",
    "for p in model_18.fc.parameters():\n",
    "    p.requires_grad = True\n",
    "    \n",
    "# for p,name in zip(model_18.parameters(),model_18.state_dict().keys()):\n",
    "#     print(name,':',p.requires_grad)\n",
    "    \n",
    "model_18 = model_18.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = optim.Adam(model_18.parameters(),weight_decay=0.001)\n",
    "\n",
    "model_18_train = train_model_N(model_18,criterion,optimizer_ft, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练二： 用模型参数初始化训练，调用L2正则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.2326 Acc: 0.5574\n",
      "val Loss: 0.8500 Acc: 0.4118\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 0.7875 Acc: 0.4918\n",
      "val Loss: 0.6612 Acc: 0.4575\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 0.7142 Acc: 0.5820\n",
      "val Loss: 3.7550 Acc: 0.5621\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.7266 Acc: 0.4713\n",
      "val Loss: 0.7786 Acc: 0.4444\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.7016 Acc: 0.5574\n",
      "val Loss: 0.8894 Acc: 0.5294\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.7062 Acc: 0.5615\n",
      "val Loss: 0.7371 Acc: 0.5621\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.7250 Acc: 0.5820\n",
      "val Loss: 0.7292 Acc: 0.5817\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.7170 Acc: 0.5656\n",
      "val Loss: 0.6568 Acc: 0.6405\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.6955 Acc: 0.5984\n",
      "val Loss: 1.1515 Acc: 0.5359\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.7114 Acc: 0.5820\n",
      "val Loss: 0.7658 Acc: 0.4837\n",
      "\n",
      "Training complete in 1m 7s\n",
      "Best val Acc: 0.640523\n"
     ]
    }
   ],
   "source": [
    "model_18 = models.resnet18(pretrained=True) \n",
    "\n",
    "model_18.fc = nn.Linear(model_18.fc.in_features, 2)\n",
    "\n",
    "model_18 = model_18.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = optim.Adam(model_18.parameters(),weight_decay=0.001)\n",
    "\n",
    "model_18_train = train_model_N(model_18,criterion,optimizer_ft, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 冻结模型参数，L1正则化训练全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model_L1(model,criterion,optimizer,num_epochs=25):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(num_epochs):        # epochs循环\n",
    "        print(\"Epoch{}/{}\".format(epoch,num_epochs-1))\n",
    "        print(\"-\"*10)\n",
    "        \n",
    "        for phase in [\"train\",\"val\"]:     # 训练模型还是检测模型\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:    # 数据装载位置\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)   # 把数据装载过去\n",
    "\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs,1)\n",
    "                    \n",
    "                    L1 = 0\n",
    "                    for param in model.parameters():\n",
    "                        L1 += torch.sum(torch.abs(param))\n",
    "                        \n",
    "                    loss = criterion(outputs,labels) + 0.001 * L1\n",
    "                    \n",
    "                    \n",
    "                    if phase == \"train\":\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            print(\"{} Loss:{:.4f} Acc:{:.4f}\".format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            if phase == \"val\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "    time_elapsed = time.time()-since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "#     model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0/9\n",
      "----------\n",
      "train Loss:168.4047 Acc:0.7008\n",
      "val Loss:168.1198 Acc:0.8954\n",
      "Epoch1/9\n",
      "----------\n",
      "train Loss:168.3342 Acc:0.7418\n",
      "val Loss:168.0437 Acc:0.9281\n",
      "Epoch2/9\n",
      "----------\n",
      "train Loss:168.2733 Acc:0.7787\n",
      "val Loss:168.0789 Acc:0.8954\n",
      "Epoch3/9\n",
      "----------\n",
      "train Loss:168.3101 Acc:0.7295\n",
      "val Loss:168.0347 Acc:0.9020\n",
      "Epoch4/9\n",
      "----------\n",
      "train Loss:168.2056 Acc:0.7951\n",
      "val Loss:168.0044 Acc:0.9346\n",
      "Epoch5/9\n",
      "----------\n",
      "train Loss:168.1903 Acc:0.8156\n",
      "val Loss:167.9893 Acc:0.9477\n",
      "Epoch6/9\n",
      "----------\n",
      "train Loss:168.2224 Acc:0.7951\n",
      "val Loss:168.0561 Acc:0.9020\n",
      "Epoch7/9\n",
      "----------\n",
      "train Loss:168.1500 Acc:0.8320\n",
      "val Loss:168.0955 Acc:0.8824\n",
      "Epoch8/9\n",
      "----------\n",
      "train Loss:168.2398 Acc:0.7869\n",
      "val Loss:168.0147 Acc:0.9412\n",
      "Epoch9/9\n",
      "----------\n",
      "train Loss:168.2346 Acc:0.7992\n",
      "val Loss:167.9974 Acc:0.9542\n",
      "Training complete in 1m 4s\n",
      "Best val Acc: 0.9542\n"
     ]
    }
   ],
   "source": [
    "model_18 = models.resnet18(pretrained=True) \n",
    "\n",
    "\n",
    "\n",
    "for p,name in zip(model_18.parameters(),model_18.state_dict().keys()):\n",
    "    p.requires_grad=False\n",
    "    \n",
    "model_18.fc = nn.Linear(model_18.fc.in_features,2)\n",
    "\n",
    "for p in model_18.fc.parameters():\n",
    "    p.requires_grad = True\n",
    "    \n",
    "# for p,name in zip(model_18.parameters(),model_18.state_dict().keys()):\n",
    "#     print(name,':',p.requires_grad)\n",
    "\n",
    "model_18 = model_18.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = optim.Adam(model_18.parameters())\n",
    "\n",
    "model_18_train = train_model_L1(model_18,criterion,optimizer_ft, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结论：\n",
    "1. 由于图像具有一定的规律，运用模型自带的参数已经解析了图像自我的规律，所以运用冻结后的参数训练，有利于提高模型的准确率\n",
    "\n",
    "2. 在本次实验中，L1正则的效果优于L2正则，主要原因是在已经冻结了模型的参数的基础上，L1的稀疏性能更好的在最后的全连接层中正确的分类。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
